\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{mwe}
%\usepackage{lingmacros}
%\usepackage{tree-dvips}
%\usepackage{blindtext}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[inline]{enumitem}

\usepackage{tikz}
\usetikzlibrary{positioning}

\begin{document}

\title{
	Project 3: Neural Transliteration
}
\author{
Gudjon Magnusson 
\and Irina Yakubinskaya 
\and Matthew Goldberg
}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Seq-to-seq model configuration}

\subsection{Q1}

\begin{figure}[!h]
\centering

\input{computation_graph}

\caption{Computation graph of the encoder-decoder model}
\label{fig_compgraph}
\end{figure}

Figure \ref{fig_compgraph} shows an overview of how the model works. The attention mechanism is simplified to fit it in the graph and is represented by the unit $\alpha$. The attention score is computed using the bilinear method. A linear transformation is applied to the encoder embedding before computing the dot product with the decoder embedding.

$h_i$ is the hidden layer at timestep $i$. It is a vector of size 256. $h_0$ is initialized as all zeros.

$x_i$ represents the letter at position $i$ in the input word. It is used to lookup a character embedding in the embedding matrix $E$. $E^{(b)}$ is the embedding matrix for the input language, in this case Bulgarian written in the Cyrillic alphabet. $E^{(b)}$ has size $256 \times 85$. 85 different characters (including start and end tokens), each represented by an embedding of size 256. $E^{(e)}$ is the embedding matrix for the output language and has size $256 \times 31$.

The output at each timestep in the encoder is concatenated to form a matrix $A$ which has size $256 \times n$, where $n$ is the length of the input word. The implementation uses a fixed size matrix with 20 columns but only the first $n$ have non-zero values. Words longer than 20 are filtered out of the training set.
The matrix $A$ is used in the decoder for the attention mechanism.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Training algorithm}

\subsection{Q2}

\paragraph{n\_iters} is the number of training iterations used to train the model. Each iteration trains the model on one randomly chosen example from the training data.

\paragraph{learning\_rate} is how much the weights are updated based on one example. Before the error gradient is subtracted from the weights, its multiplied by \texttt{learning\_rate}. Slowing down the learning (with $learning\_rate < 1$) makes the process more stable and prevents one bad example from ruining good weights.


\subsection{Q3}

To choose values for \texttt{learning\_rate}, commonly refereed to as $\alpha$, and \texttt{n\_iter} we ran two experiments. 
First we trained the model with a few different values for \texttt{learning\_rate} and tracked the loss for each iteration. We would like to find a value that decays the loss quickly and appears stable over a large number of iterations.
Figure \ref{fig_learning_rate} shows the comparison of learning curves for a few different values of $\alpha$. In general higher values of $\alpha$ lead to faster convergence, but we found that the training becomes unstable when we raise $\alpha$ to 0.02 or higher. 

Next we trained the model and tracked how the average edit distance on a small held out dataset changed after every 100 iterations. For this we fixed \texttt{learning\_rate} to something that appeared reasonable, we choose 0.01. The goal is to see how long we can train the model before over-fitting.

\begin{figure}[t!]
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/learning_rate2.eps}
        \caption{Learning curves for different $\alpha$}
        \label{fig_learning_rate}
    \end{subfigure}
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/n_iter2.eps}
        \caption{Average edit distance}
        \label{fig_n_iter}
    \end{subfigure}
	\caption{Experiments with \texttt{learning\_rate} and \texttt{n\_iter}}
	\label{fig_train_param}
\end{figure}

We expected to see the edit distance to get lower at first and then start to rise again at some point, but as can be seen in figure \ref{fig_n_iter} the edit distance does not go back up. The score appears to plateau after about 15 thousand iterations, so there is little benefit to train past that.

\section{Understanding teacher forcing}

\subsection{Q4}

\texttt{teacher\_forcing} is the probability of using the "teacher" for each training sample. If it is set to 1, the teacher is used on every sample.

When the teacher is used for training mistakes that the model makes are ignored. At each time step, if the model makes a mistake, it contributes to loss but the model prediction is replaced with the correct letter. The training moves to the next time step as if no mistake was made.

If the teacher is used too much it can lead to an unstable model that learns how to create good looking outputs rather than good transliterations. I.e it's better to get the correct transliteration with one spelling mistake, than the wrong word perfectly spelled.

\subsection{Q5}
 
With higher values of \texttt{teacher\_forcing} the loss decays faster during training. This can be seen in figure \ref{fig_teacher}, which shows the learning curve for 3 different values of \texttt{teacher\_forcing}.

Although it converges faster with higher \texttt{teacher\_forcing}, the average edit distance is lower for the model that was trained with a lower \texttt{teacher\_forcing}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{img/teacher_forcing2.eps}
    \caption{Learning curves for different values of \texttt{teacher\_forcing}}
    \label{fig_teacher}
\end{figure}

\begin{center}
 \begin{tabular}{|c || c | c | c | c | c |} 
 \hline
 \texttt{teacher\_forcing} & 0.0 & 0.1 & 0.5 & 0.9 & 1.0 \\ [0.5ex] 
 \hline
 Avg Edit Dist & 1.12 & 1.09 & 0.77 & 2.43 & 1.16 \\ 
 \hline
\end{tabular}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Impact of attention mechanism}

\subsection{Q6}

Transliteration is not a simple one-to-one character replacement, there is a complicated relationship between the input and output characters. The attention mechanism helps to capture that relationship. At each timestep in prediction the model can focus on the input characters that are most likely to influence the output.


\subsection{Q8}

Figure \ref{fig_attention} shows the learning curve for the model with and without the attention mechanism. The model converges faster with attention, and it performs better when evaluated on unseen data. The average edit distance is about 2.4 without attention and 1.72 with attention.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{img/attention2.eps}
    \caption{Learning curves with and without the attention mechanism}
    \label{fig_attention}
\end{figure}

%with attention 1.72
%without 2.4

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Something new}

\subsection{Q9}
\textit{Briefly explain what you did}

To improve the model we experimented with two modifications. Minibatch training and beam search decoding. The goal with these modifications is to stabilize and speed up the training, and improve the quality of the outputs.

\subsubsection{Minibatch Training}

We found that training with the original model was sometimes unstable. The loss would go down for a while but then briefly spike up again. This sometimes lead to a bad model if training happened to stop in one of those spikes. This could be addressed be by using a smarter stopping criteria or lowering the learning rate. We choose to address this by implementing mini batching.

In addition to gaining stability we hoped to gain a noticeable increase in training speed thanks to the performance boost of grouping calculations together in matrix to matrix operations.

\subsubsection{Beam Search Decoder}

The original model produces output words one character at a time in a greedy fashion.
We found that the output was often either fully correct or started correctly and then deviated completely after some point in the word. This lead us to believe that maybe the decoder was running into trouble because of one bad prediction, and, maybe it could be improved by considering more than one option at each timestep.

To improve the chance of finding globally optimal prediction for the output word we implemented beam search. Instead of just using the locally optimal choice at each timestep, it graph that keeps tracks the $b$ best hypothesis for the output word.

\subsection{Q10}
\textit{Design an experiment to test whether your solution successfully addresses the problem}

\subsubsection{Minibatch Training}

\subsubsection{Beam Search Decoder}

To test how the beam search decoder affects the output quality we train the model as we do normally. We then produce outputs with and without the beam decoder, and with a few different beam widths. We do this every thousand iterations as train the model and compare the average edit distance between the predicted output and the target output.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{img/beam_width2.eps}
    \caption{The effect of beam width on edit distance}
    \label{fig_beam_width}
\end{figure}

Unfortunately beam search decoder has an extreme bias for short words. We knew this might be the case but had hoped that it would still show an improvement with low values of $b$. As shown in figure \ref{fig_beam_width}, even with $b=2$ the quality suffers. As expected $b=1$ behaves the same as greedy decoding.

\subsection{Q11}
\textit{}


\end{document}